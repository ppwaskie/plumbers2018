% This file is isea.tex.  It contains the formatting instructions for and acts as a template for submissions to ISEA 2015.  It is based on the ICCC  formats and instructions.  It uses the files isea.sty, isea.bst and isea.bib, the first two of which also borrow from AAAI IJCAI formats and instructions.
% Modified from ICCC.tex by B. Bogart

\documentclass[letterpaper]{article}
\usepackage{isea}
\usepackage[pdftex]{graphicx}
\usepackage{times}
\usepackage{helvet}
\usepackage{courier}
\usepackage[numbers]{natbib}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\pdfinfo{
/Title (Accelerating Real-World XDP Programs Using HW-based Hints)
/Author (Peter P. Waskiewicz Jr)}
% The file isea.sty is the style file for ISEA 2015 proceedings.
%
\title{Accelerating Real-World XDP Programs Using HW-based Hints}
\author{Peter P. Waskiewicz Jr. \\ Intel \\ Hillsboro, OR, USA \\ peter.waskiewicz.jr@intel.com
\And Neerav Parikh \\ Intel \\ Hillsboro, OR, USA \\ neerav.parikh@intel.com
\And Saeed Mahameed \\ Mellanox \\ San Francisco, CA, USA \\ saeedm@mellanox.com
\newline
\newline
}
\setcounter{secnumdepth}{0}

\begin{document} 
\maketitle
\begin{abstract}
This talk is a continuation of the initial XDP HW-based hints work presented at NetDev 2.2 in Seoul, South Korea.

It will will start with focus on showcasing new prototypes to allow an XDP program to request required HW-generated metadata hints from a NIC. The talk will show how the hints are generated by the NIC and what are the performance characteristics for various XDP applications. We also want to demonstrate how such a metadata can be helpful for applications that use AF\_XDP sockets.

The talk with then discuss planned upstreaming thoughts, and look to generate more discussion around implementation details, programming flows, etc., with the larger audience from the community.
\end{abstract}

\section{Keywords}

networking, kernel, ebpf, xdp, offloads, performance

\section{Introduction}
XDP continues to evolve and grow in its capabilities and features for a growing number of workload types.  In order to continue maximizing the efficiency and performance of XDP, focus on offloading pieces of the processing to hardware is key.  Initially shared at NetDev 2.2 in Seoul, South Korea in Nov. 2017 \cite{xdp-acceleration-2017}, the initial proposal explored a simple theoretical case of improving XDP performance using hardware-provided metadata.  This paper will continue that exploration, which will cover:
\begin{itemize}
\item How does this HW-provided metadata help real-world XDP applications?
\item What happens to performance when hardware is actually generating this metadata inline with packet data, versus through descriptors or synthesized in the driver itself?
\item What new applications should be focused on next?
\item How can BPF Type Format (BTF) be used to program which HW-provided metadata XDP applications wish an underlying driver/device to generate and present to the application?
\end{itemize}

\section{HW Offloaded Hints From Device Driver - Recap} 
In the initial work and testing done to try and improve XDP performance using HW-generated hints, the focus was mostly on the kernel and driver interfaces that would need to change.  The introduction of the {\small \texttt{data\_meta}} pointer inside the {\small \texttt{xdp\_buff}} was the start of the required groundwork.  This allowed a driver to begin placing data, either via {\small \texttt{memcpy()}} or through other ways (e.g. DMA directly if the device supported DMA to pre-packet payloads).
\newline
\newline
What the data showed is that there was promise for this work.  In Figure \ref{xdp-program-performance}, the XDP\_HINTS program, which is a modified version of the simple XDP1 sample program, would look at the packet type as reported by the driver.  It would then decide to drop the packet or not if it matched a certain packet type.  The difference here though is the XDP program itself didn't parse the packet header, rather the program consumed the metadata provided by the driver and ultimately the hardware.  This led to a large performance increase of the XDP application, where it eventually hit a hardware limit in the test setup.

\begin{figure}[h]
\includegraphics[width=3.31in]{xdp-programs-performance.png}
\caption{XDP With and Without HW Hints}
\label{xdp-program-performance}
\end{figure}

\section{Real-World Application: Layer 4 Load Balancer}
One of the most effective applications that XDP can support is a network load balancer.  Facebook released their Katran load balancer as open-source \cite{katran-2018} which at its core is an XDP application.  This is what they employ as their main edge load balancer, which showcases how effective XDP is by running one of the largest data centers in the world.
\newline
\newline
Using a simple version of this layer 4 (L4) load balancer, instrumentation was done to the i40e driver \cite{xdp-patches-2018} and to the simple L4 LB to consume metadata that was “generated” by the device.  In this scenario, the metadata was inserted into the {\small \texttt{data\_meta}} section of the {\small \texttt{xdp\_buff}} by the driver via {\small \texttt{memcpy()}}, as the i40e device in use does not currently have the ability to DMA metadata prior to packet data.  So consider this metadata partially synthesized.
\newline
\newline
The metadata that was synthesized and provided was a combination of packet type (as identified by the i40e hardware), and a tuple that included TCP/UDP source/destination ports, IP addresses, and the packet hash index (RSS) generated by hardware.

\subsection{Without state or connection tracking}

\begin{figure}[h]
\includegraphics[width=3.31in]{l4-perf-no-tracking.png}
\caption{L4 LB with Hints, No State Tracking}
\label{l4-perf-no-tracking}
\end{figure}

In Figure \ref{l4-perf-no-tracking}, the data is showing performance of the L4 LB across various configurations.
\newline
\newline
In Type 1 Hints, the metadata being provided by the driver is the packet type (e.g. IPv4/TCP, IPv6/UDP, etc.).  These packet types are what the i40e hardware identifies off the wire, and are not yet standardized in the network stack.  However, the L4 LB is modified in this situation to match the i40e packet types.
\newline
\newline
In Type 2 Hints, the metadata being provided by the driver is metadata from Type 1, but then also includes the Source/Destination Port, Source/Destination IP Addresses, and the packet index type (RSS) generated by the hardware.
\newline
\newline
These tests are running in a stateless configuration, meaning, the XDP application is not maintaining any connection state tracking.  In other words, there are no map entries being created for new connections, and map entries being destroyed on gracefully closed connections.  This eliminates a large amount of overhead, and can be seen in the scalability of the testing a 4 queues with Type 2 hints, running at approximately 14 million packets/second.  Compare this to no driver-provided hints with 4 queues, and the test can run at roughly half the performance, or 7 million packets/second.

\subsection{With State and Connection Tracking}

\begin{figure}[h]
\includegraphics[width=3.31in]{l4-perf-with-tracking.png}
\caption{L4 LB with Hints, With State Tracking}
\label{l4-perf-with-tracking}
\end{figure}
In Figure \ref{l4-perf-with-tracking}, a more real-world scenario is presented for the L4 LB.  This includes the tracking of connections and state of those connections, which will require many more accesses to the eBPF maps.  The metadata presented is the same as in the tests without connection and state tracking.
\newline
\newline
Between the two main tests, each using 4 queues, Type 2 hints and no hints at all perform the same, approximately 8.1 million packets/second.  Unfortunately the large number of map hits are the majority of the execution pipeline, where the HW-generated metadata hints cannot help in this situation.  The gains seen in the data processing of the actual packet are just a fraction of the overall cost to the load balancer.  In other words, with state tracking, a L4 LB with this approach using these tests with synthesized hints will not see much benefit or performance gain.

\subsection{Projected Performance Increase}
In the stateless and stateful L4 LB tests, the method of generating the metadata inside the {\small \texttt{xdp\_buff}} struct was to {\small \texttt{memcpy()}} the data into the {\small \texttt{data\_meta}} section of the payload.  This isn’t the most ideal situation, since this can more than certainly cause cache misses, and incur additional overhead while trying to process data.
\newline
\newline
\begin{figure}[h]
\includegraphics[width=3.31in]{xdp-projected-perf.png}
\caption{Projected Performance with Inline Metadata}
\label{xdp-projected-performance}
\end{figure}
As seen in Figure \ref{xdp-projected-performance}, extrapolation can be done by breaking down cycle counts for the stateful L4 LB tests.  Observing the number of cycles being spent in the i40e driver to synthesize the metadata, and the subsequent {\small \texttt{memcpy()’s}} of information into the {\small \texttt{xdp\_buff}} metadata payload, cycles are available to save if the underlying hardware placed the metadata directly into the {\small \texttt{xdp\_buff}} during DMA.  With the dataset from the stateful tests, that could provide an addition 6-7\% throughput in the Type 2 Hints with 4 queues test.

\section{Future Application Research}
Given the current challenges of little to no gain in performance using Type 2 hints with XDP applications with stateful tracking, additional applications should be considered.  Distributed Denial of Service (DDoS) attacks are very effectively mitigated using XDP, since there is no state tracking outside of blacklist (or whitelist) ranges to drop.  The XDP application itself does not require maintenance of the DDoS blacklist maps, and only requires lookups to decide the packet action, where stateful trackers will also change and update maps.
\newline
\newline
Future testing is planned for these types of applications, where the Type 2 Hints with no state tracking is showing significant increases in performance with the L4 LB (100\% increase).

\section{BTF Integration: Expressing Requested Hints}
At NetDev 2.2, that work explored changes to LLVM and the entire toolchain to include a requested hints section in the eBPF binary.  This would be passed through the eBPF load mechanisms, and then eventually intercepted and passed to the underlying driver to program the hardware to generate the requested hints.  This was purely a proposal to get discussion going.
\newline
\newline
With the recently-added BPF Type Format (BTF) to the Linux kernel \cite{btf-patches-2018}, this tooling is now mostly in place.  This allows different metadata structures to be defined through the BTF framework.  However, this is just the top half of the solution.
\newline
\newline

\section{XDP Metadata User API}
Adding support to XDP meta data to be passed from a driver to a user XDP program is as easy as filling the headroom part of each Rx page/buffer with meta data fields and adjusting the meta data pointer accordingly. Then the user space program just accesses the \small{\texttt{xdp->data\_meta}} field.  For example:
\begin{itemize}
	\item Driver on XDP Rx packet:
	\newline
	{\scriptsize \texttt{xdp.data\_meta  = xdp->data - sizeof(meta\_data);}}
	{\scriptsize \texttt{*xdp.data\_meta = meta\_data;}}
	\newline
	\item User space XDP program:
	\newline
	{\scriptsize \texttt{meta\_data = (struct meta\_data*)(long)ctx->data\_meta;}}
\end{itemize}

The problem with such approach is that the XDP program needs to know the type of \small{\texttt{struct meta\_data}} which will be written by a driver/hardware, and we would want to keep generic XDP programs to work with multiple NICs.
\newline
\newline
In order to keep a generic XDP programs and yet preserve driver and XDP program max performance we must satisfy some requirements:
\begin{itemize}
	\item Generic Meta data types, same xdp program can work with different drivers that support same subset of meta data types.
	\item Dynamic meta data settings, in order to work with the minimal set of metadata(s) in a XDP program, XDP program will request from the driver to provide only the required meta data. \textsubscript{* For maximum performance, driver/HW will need to only}
	\textsubscript{copy the required metadata fields to the meta data buffer, and not more.}
	\item User space must know how to access each meta data field with direct access, no lookup and preferably no checking if the meta data was actually set by the driver.
\end{itemize}

\subsection{Suggested Solutions}

\begin{itemize}
	\item Well known XDP meta data types \cite{mlx-rx-accel-patches-2018} \cite{mlx-xdp-meta-patches-2018}, defined by the kernel as enum (flag) and each will have a well-known type(enum) size and format.

	\begin{itemize}
		\item A program can request any subset of well-known meta data fields from driver
		\item The driver will fill only the requested meta data fields into the meta data buffer in a pre-defined order (ascending order according to the requested meta data enums)
		\item A pre-known order is required so the XDP program will know how to read (format) and where to find (offset) the meta data fields, without the need for type checking or lookups.
	\end{itemize}
	A well-known meta data field can be packet hash (32 bit hash, u8 type), VLAN (16 bit), flow mark (32 bit), csum complete (16 bit), etc. 
	\newline
	\newline
	The driver and the XDP program will maintain a global "offset array", an array that defines which meta data is present (requested by the program).  It will also know where (which offset), enabling the driver and the XDP program to always agree on where to write or read a specific meta data field.  For example:
	\newline
	{\scriptsize \texttt{flow\_mark = xdp->data\_meta + offset\_array[XDP\_META\_FLOW\_MARK];}}
	\newline
	One major problem with this approach is the need to have a well defined meta data fields, managed by the kernel XDP UAPI, which will not easily allow vendor specific fields to be passed by driver or HW to XDP programs without the need to expose them to the kernel first.

\item BTF: BPF Type Format \cite{btf-patches-2018}
	The driver/FW/HW can report a BTF definition of the meta data structure that will fill the xdp meta data buffer. A BTF will be returned in C-header format, as proposed by Alexei Starovoitov and Daniel Borkmann \cite{mlx-rx-infra-patches-2018} below :
	\begin{itemize}
		\item A new query as in XDP\_QUERY\_META\_BTF is likely required for user space, implemented in \small{\texttt{ndo\_bpf}} callback and then exported e.g. via the bpf(2) system call, or netlink such that bpftool can generate BTF -> C from there for the program to include later in compilation.
		\item Driver+firmware keep layout of the metadata in BTF format
		\item \small{\texttt{'bpftool read-metadata-desc eth0 > md\_desc.h'}} command will query the driver and generate normal C header file based on BTF in the given NIC
		\item User does \small{\texttt{\#include "md\_desc.h"}} and eBPF program can access \small{\texttt{md->csum}} or \small{\texttt{md->hash}} with direct single load out of metadata area in front of the packet
		\item LLVM compiles eBPF program and records how program is doing this \small{\texttt{md->csum}} accesses in BTF format as well (the compiler will be keeping such records for \small{\texttt{\_\_sk\_buff}} and all other structs too, but that's separate discussion)
		\item During \small{\texttt{sys\_bpf(prog\_load)}} the kernel checks (via supplied BTF) that the way the program accesses metadata (and other structs) matches BTF from the driver, so no surprises if driver+firmware got updated, but program is not recompiled.
		\item Every NIC can have their own layout of metadata and its own meaning of the fields, but would be good to standardize at least a few common fields like hash
	\end{itemize}
  
Pros: of BTF approach compared to offsets approach
1. Meta data generic, allows vendor specific offloads without the need for
kernel support.
2. Meta data access is optimized for compile time rather than using offsets at
run time.

Cons:
1. Recompiling the XDP program per machine, interface type, driver version, FW
version .. 
2. Standardizing some fields is up to naming conventions of fields between the
different vendors.
3. It will be impossible to compile same xdp program on two different NIC vendors
which is using filed 1 of vendor 1 and field 2 of vendor 2.

\end{itemize}

\section{Conclusion}
Initial thoughts whether or not XDP could benefit from HW-generated metadata in real-world applications are proven that this is the case.  Utilizing hardware, especially as it becomes smarter and more flexible, will continue pushing XDP’s performance envelope.  In the span of a year, this has moved from a purely theoretical hope of improvement, to showing it can help in real-world applications.  The next steps needed to make this part of the XDP core is the ability to flexibly program the driver and hardware using the BTF framework.  And as hardware capabilities allow the DMA of metadata as part of the packet payloads, then more XDP applications will benefit from performance gains across the board.

\section{Acknowledgments}
We would like to acknowledge the Linux Plumbers Networking Track selection committee for inviting us to submit and present this paper.

\bibliographystyle{xdp-plumbers-2018}
\bibliography{xdp-plumb ers-2018}

\section{Author Biographies}
Peter Waskiewicz Jr (PJ) is a Senior Linux Kernel Engineer in the Networking Division of Intel's Communications Group. He has maintained and helped create the igb, ixgbe, and i40e wired Ethernet network drivers, the initial Tx multiqueue support in the Linux kernel network stack, and added Data Center Bridging support to the Linux kernel. He also worked in Intel's Open Source Technology Center on the x86 kernel tree, enabling advanced features in the Broadwell and Skylake microarchitectures. Prior to returning to Intel, PJ was a Principal Engineer at NetApp in the SolidFire division, where he was the chief Linux kernel and networking architect for the SolidFire scale-out cloud storage platform.
\newline
\newline
Neerav Parikh is a Software Architect with Intel's Connectivity Group focusing on Ethernet Networking Software. He has worked on Intel's ixgbe and i40e Linux device drivers, focusing on features related to FCoE, DCB, and QoS. Prior to joining Intel, Neerav worked as a Technical Architect enabling SAS/SATA/FC-based Storage software products.
\newline
\newline
Saeed Mahameed is a Staff Software Engineer at Mellanox Technologies, working on the Mellanox network drivers, and various aspects of the Linux Kernel Network Stack.

\end{document}
